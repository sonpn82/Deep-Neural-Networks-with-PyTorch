{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# IBM Developer Skills Network"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Convolution"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Convolution is a linear operation similar to a linear equation, dot product, or matrix multiplication. Convolution has several advantages for analyzing images. As discussed in the video, convolution preserves the relationship between elements, and it requires fewer parameters than other methods.  \r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can see the relationship between the different methods that you learned:\r\n",
                "\r\n",
                "$$linear \\ equation :y=wx+b$$\r\n",
                "$$linear\\ equation\\ with\\ multiple \\ variables \\ where \\ \\mathbf{x} \\ is \\ a \\ vector \\ \\mathbf{y}=\\mathbf{wx}+b$$\r\n",
                "$$ \\ matrix\\ multiplication \\ where \\ \\mathbf{X} \\ in \\ a \\ matrix \\ \\mathbf{y}=\\mathbf{wX}+\\mathbf{b} $$\r\n",
                "$$\\ convolution \\ where \\ \\mathbf{X} \\ and \\ \\mathbf{Y} \\ is \\ a \\ tensor \\  \\mathbf{Y}=\\mathbf{w}*\\mathbf{X}+\\mathbf{b}$$\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "In convolution, the parameter <b>w</b> is called a kernel. You can perform convolution on images where you let the variable image denote the variable X and w denote the parameter.\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import torch\r\n",
                "import torch.nn as nn\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "import numpy as np\r\n",
                "from scipy import ndimage, misc"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\r\n",
                "conv"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 2
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# give values to parameters (already randomly generated)\r\n",
                "conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\r\n",
                "conv.state_dict()['bias'][0]=0.0\r\n",
                "conv.state_dict()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "OrderedDict([('weight',\n",
                            "              tensor([[[[ 1.,  0., -1.],\n",
                            "                        [ 2.,  0., -2.],\n",
                            "                        [ 1.,  0., -1.]]]])),\n",
                            "             ('bias', tensor([0.]))])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Create a dummy tensor to represent an image. The shape of the image is (1,1,5,5) where:\r\n",
                "\r\n",
                "(number of inputs, number of channels, number of rows, number of columns ) \r\n",
                "\r\n",
                "Set the third column to 1:\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "image = torch.zeros(1, 1, 5, 5)\r\n",
                "image[0, 0, :,2] = 1\r\n",
                "image"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[[[0., 0., 1., 0., 0.],\n",
                            "          [0., 0., 1., 0., 0.],\n",
                            "          [0., 0., 1., 0., 0.],\n",
                            "          [0., 0., 1., 0., 0.],\n",
                            "          [0., 0., 1., 0., 0.]]]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Call the object <code>conv</code> on the tensor <code>image</code> as an input to perform the convolution and assign the result to the tensor <code>z</code>. \r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "z = conv(image)\r\n",
                "z"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[[[-4.,  0.,  4.],\n",
                            "          [-4.,  0.,  4.],\n",
                            "          [-4.,  0.,  4.]]]], grad_fn=<ThnnConv2DBackward>)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 5
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Determining the Size of the Output"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The size of the output is an important parameter. In this lab, you will assume square images. For rectangular images, the same formula can be used in for each dimension independently.  \r\n",
                "\r\n",
                "Let M be the size of the input and K be the size of the kernel. The size of the output is given by the following formula:\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "$$M_{new}=M-K+1$$"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# Create a kernel of size 2\r\n",
                "K=2\r\n",
                "conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=K)\r\n",
                "conv1.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\r\n",
                "conv1.state_dict()['bias'][0]=0.0\r\n",
                "conv1.state_dict()\r\n",
                "conv1"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "# Create an image of size 4\r\n",
                "M=4\r\n",
                "image1 = torch.ones(1,1,M,M)\r\n",
                "image1"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[[[1., 1., 1., 1.],\n",
                            "          [1., 1., 1., 1.],\n",
                            "          [1., 1., 1., 1.],\n",
                            "          [1., 1., 1., 1.]]]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "# Perform the convolution and verify the size\r\n",
                "z1 = conv1(image1)\r\n",
                "print('z1:', z1)\r\n",
                "print('shape:', z1.shape[2:4])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "z1: tensor([[[[4., 4., 4.],\n",
                        "          [4., 4., 4.],\n",
                        "          [4., 4., 4.]]]], grad_fn=<ThnnConv2DBackward>)\n",
                        "shape: torch.Size([3, 3])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Stride parameter"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "conv3 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=2)\r\n",
                "\r\n",
                "conv3.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\r\n",
                "conv3.state_dict()['bias'][0]=0.0\r\n",
                "conv3.state_dict()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "OrderedDict([('weight',\n",
                            "              tensor([[[[1., 1.],\n",
                            "                        [1., 1.]]]])),\n",
                            "             ('bias', tensor([0.]))])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "z3=conv3(image1)\r\n",
                "\r\n",
                "print(\"z3:\",z3)\r\n",
                "print(\"shape:\",z3.shape[2:4])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "z3: tensor([[[[4., 4.],\n",
                        "          [4., 4.]]]], grad_fn=<ThnnConv2DBackward>)\n",
                        "shape: torch.Size([2, 2])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Zero Padding"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "image1"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[[[1., 1., 1., 1.],\n",
                            "          [1., 1., 1., 1.],\n",
                            "          [1., 1., 1., 1.],\n",
                            "          [1., 1., 1., 1.]]]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 12
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "conv4 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\r\n",
                "conv4.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\r\n",
                "conv4.state_dict()['bias'][0]=0.0\r\n",
                "conv4.state_dict()\r\n",
                "z4=conv4(image1)\r\n",
                "print(\"z4:\",z4)\r\n",
                "print(\"z4:\",z4.shape[2:4])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "z4: tensor([[[[4.]]]], grad_fn=<ThnnConv2DBackward>)\n",
                        "z4: torch.Size([1, 1])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can add rows and columns of zeros around the image. This is called padding. In the constructor <code>Conv2d</code>, you specify the number of rows or columns of zeros that you want to add with the parameter padding. \r\n",
                "\r\n",
                "For a square image, you merely pad an extra column of zeros to the first column and the last column. Repeat the process for the rows. As a result, for a square image, the width and height is the original size plus 2 x the number of padding elements specified. You can then determine the size of the output after subsequent operations accordingly as shown in the following equation where you determine the size of an image after padding and then applying a convolutions kernel of size K.\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "conv5 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=3, padding=1)\r\n",
                "\r\n",
                "conv5.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\r\n",
                "conv5.state_dict()['bias'][0]=0.0\r\n",
                "conv5.state_dict()\r\n",
                "z5=conv5(image1)\r\n",
                "print(\"z5:\",z5)\r\n",
                "print(\"z5:\",z4.shape[2:4])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "z5: tensor([[[[1., 2.],\n",
                        "          [2., 4.]]]], grad_fn=<ThnnConv2DBackward>)\n",
                        "z5: torch.Size([1, 1])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# Practice\r\n",
                "Image = torch.randn((1,1,4,4))\r\n",
                "Image"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[[[ 0.6695, -0.4136,  0.7831,  0.7468],\n",
                            "          [-0.2322,  1.0589,  0.6779, -0.3197],\n",
                            "          [ 1.1120, -0.2007,  0.1384,  0.4766],\n",
                            "          [ 0.7963, -0.3792, -0.9433, -1.0325]]]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\r\n",
                "conv.state_dict()['weight'][0][0]=torch.tensor([[0,0,0],[0,0,0],[0,0.0,0]])\r\n",
                "conv.state_dict()['bias'][0]=0.0\r\n",
                "\r\n",
                "zx = conv(Image)\r\n",
                "zx"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[[[0., 0.],\n",
                            "          [0., 0.]]]], grad_fn=<ThnnConv2DBackward>)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 16
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}